### Understanding the Data

```py
import pandas as pd
```
```py
advertising = pd.read_csv('/content/tvmarketing.csv')

# Column TV - advertising budget spent on TV marketing 
# Column Sales - the actual sales (how much we have sold/sales)
```
```py
advertising.head() # first 5 rows
```
```py
advertising.describe()
```
```py
# Visualising the Dataset

import seaborn as sns
```
```py
sns.regplot(data=advertising, x='TV',y="Sales",ci=99,marker="x",color="black",line_kws=dict(color="green"))
```
```py
# Pearson correlation koeficient

advertising.corr() #calculating correlation for the whole dataframe
```
### Correlation reminder

Weak Correlation: 0 to ±0.3

Moderate Correlation: ±0.3 to ±0.7

Strong Correlation: ±0.7 to ±1


```py
advertising['TV'].corr(advertising['Sales']) # positive, strong correlation
```
### SUPERVISED VS UNSUPERVISED learning

* Supervised learning: You have input data and know the correct answers (labels or targets). The model learns to map inputs to the correct outputs. Think of it like having a teacher who shows you the right answers as you learn.

* Unsupervised learning: You only have input data with no correct answers. The model tries to find patterns or groups in the data on its own. It's like exploring a puzzle without knowing what the final picture looks like.

Supervised learning is like studying with an answer key, while unsupervised learning is like figuring things out without any hints.

Supervised learning models would have a baseline understanding of what the correct output values should be.

### IN SHORT: In unsupervised - you don't have labeled data, in supervised - the data is labeled. 

![Screenshot 2024-08-17 at 10 28 14](https://github.com/user-attachments/assets/b639e483-89f0-4785-9fdb-4d73670d2d7d)

### Scikit library:
Simple and efficient tools for predictive data analysis
Accessible to everybody, and reusable in various contexts
Built on NumPy, SciPy, and matplotlib
Open source, commercially usable - BSD license

https://scikit-learn.org/stable/
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html

```py
# Preparing X and Y 

X = advertising.drop('Sales', axis=1) # independent variable 

X.head()
```
```py
Y = advertising.drop('TV', axis=1) # the outcome, dependent

Y.head()
```

# Splitting data into Train/Test sets

```py
from sklearn.model_selection import train_test_split
```
```py
X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.7) # usually X is written as capital letter and y as lower case.

# 0.7 specify that 70% of the data should be used for training and 30% for testing.

X_train
```
### Column/ROW
* COLUMN - also called a FEATURE
* ROW - also called an OBSERVATION

## Train the model 

```py
from sklearn.linear_model import LinearRegression # Import the model
```
```py
model = LinearRegression() # Initialize the model
```
```py
model.fit(X_train, y_train) # Fit the model
```

### Y = mX + b

Here's what each part represents:
Y: The dependent variable or the output value you want to predict.
X: The independent variable or the input value you use to make the prediction.
m: The slope of the line, which indicates how much Y changes for a unit change in X
b: The y-intercept, which is the value of Y when X is 0. It represents where the line crosses the Y-axis.

```py
print(f'Model intercept is: {model.intercept_}')
print(f'Model coeficient is: {model.coef_}')
```
```py
my_new_budget = [[50]] # double square brackets create 2D Array Dataset

my_predicted_sales = model.predict(my_new_budget)

print(f'When the new budget is {my_new_budget[0][0]}, then predicted sales are: {round (my_predicted_sales[0][0])}') # the [0][0] gets rid of the double [[]]

X_test.head()
```
```py
y_pred = model.predict(X_test)
```
```py
final_df = pd.DataFrame()
final_df['TV'] = X_test
final_df['Actual_sales'] = y_test
final_df['Predicted_sales'] = y_pred

final_df.head()
```
```py
final_df['ABS_Error_Actual_vs_Predicted'] = abs(final_df['Actual_sales']-final_df['Predicted_sales'])
```
```py
final_df.head()
```

### Visualising the result ###

```py
final_df.reset_index(inplace=True)
final_df.head()
```

```py
import matplotlib.pyplot as plt

plt.plot(final_df.index, final_df.ABS_Error_Actual_vs_Predicted)
plt.title('Actual vc Predicted ABS')
plt.xlabel('Index')
plt.ylabel('Sales')
plt.axhline(final_df['ABS_Error_Actual_vs_Predicted'].mean(), linestyle='dashed', color='red')
```
* intercept is the point where the model's prediction line crosses the y-axis on a graph of real data
* coefficient is the slope of the line

##  MEAN ABSOLUTE ERROR :

Mean Absolute Error (MAE) is a metric used to measure the average magnitude of the errors in a set of predictions, without considering their direction (i.e., whether they are positive or negative). The steps are as follows:

Calculate the error: Find the difference between each actual value and the corresponding predicted value. This is also known as the residual or error.

Convert to absolute terms: Take the absolute value of each error to remove any negative signs. This step ensures that all errors are treated as positive values, so you measure the magnitude of errors, regardless of whether the prediction was higher or lower than the actual value.

Sum them up: Add all the absolute errors together to get the total error.

Average the errors: Divide the total error by the number of observations (data points) to get the mean absolute error.

! MAE calculates the average of the absolute differences between actual and predicted values, providing a straightforward way to understand prediction errors.

```py
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)

print(f'Mean ABS error is : {mae}')  # average distance between real and predicted data
```

![Screenshot 2024-08-24 at 10 35 32](https://github.com/user-attachments/assets/71d2b007-6981-46c8-bc72-8b7bbc258e33)

* The lower the MEA, the more accurate our model is

## R2 value ##

The R² score ranges from 0 to 1 for a well-fitting model, but it can also be negative if the model is performing worse than a simple horizontal line representing the mean of the target variable.

R² = 1: Perfect prediction! The line you drew predicts every single point exactly as it is. The model explains 100% of the variation in the data.
R² = 0: Your model is as good as just predicting the average of all the points. The model explains none of the variation in the data.
R² < 0: Your model is worse than just predicting the average. It means your predictions are pretty off.

```py
from sklearn.metrics import r2_score, mean_squared_error

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'MSE is: {mse}')
print(f'r2 is: {r2}')
```



## TEAMWORK:

* What is an error rate?

The error rate is the percentage of incorrect predictions made by a model out of the total predictions. Lower error rates mean better model performance.

* Where could you use other machine-learning models?

Different machine learning models are used based on the problem: classification (e.g., spam detection), regression (e.g., predicting house prices), clustering (e.g., customer segmentation), etc.

* What is the difference between supervised and unsupervised training?

Supervised learning uses labeled data (with correct answers) to train models, while unsupervised learning works with unlabeled data, trying to find patterns or groupings on its own.

* How to import different models from the scikit-learn package?

Use from sklearn.model import ModelName to import models. For example, from sklearn.ensemble import RandomForestClassifier imports a random forest classifier.

* How can you evaluate the performance of a machine learning model in scikit-learn?

Use metrics like accuracy, precision, recall, F1-score, and confusion matrices to evaluate a model's performance.

* What metrics are commonly used for evaluation?

Common metrics include accuracy (overall correctness), precision (correct positive predictions), recall (capturing actual positives), F1-score (balance between precision and recall), and ROC-AUC score (for binary classification).

* What is model overfitting, and how can it be prevented?

Overfitting occurs when a model performs well on training data but poorly on new, unseen data. It can be prevented by using techniques like cross-validation, regularization, and simplifying the model (e.g., pruning in decision trees).
