Lesson 23 Topic: Confusion matrix & Clustering

## Clustering

✨ Utilities to load popular datasets and artificial data generators -> https://scikit-learn.org/dev/api/sklearn.datasets.html

✨ Unsupervised VS Supervised learning

![Screenshot 2024-09-28 at 10 03 24](https://github.com/user-attachments/assets/6fba381a-fc2e-4b98-a761-078eac56782b)


* Dots being in the same color means that input data isn't labeled. In supervised learning - the data is labeled, as we see here from two different colors and grouping.
* When new points come in, the model will not retrain on those points. You need to retrain the model yourself. 

✨ Visualising decision boundaries 

![Screenshot 2024-09-28 at 10 52 17](https://github.com/user-attachments/assets/8a90222d-68b1-403d-b4c3-42de1d13cb91)


https://stackoverflow.com/questions/22294241/plotting-a-decision-boundary-separating-2-classes-using-matplotlibs-pyplot

✨ A practical example of where this would be needed - Spotify, using functionality for listening to random songs or Netflix with movie suggestions etc.

✨ Unsupervised learning example (clustering) -> Using synthetic data here (not real data, but rather something made for learning purposes)

```py
from sklearn.datasets import make_blobs # creating synthetic dataset
```
```py
X, y = make_blobs(n_samples=100000, centers=6, cluster_std=0.80)

# X[:, 0] x coordinate
# X[:, 1] y coordinate
# y is the cluster of the point
```
```py
import matplotlib.pyplot as plt

plt.scatter(X[:, 0], X[:, 1], c='grey', edgecolors='k')
```
```py
from sklearn.cluster import KMeans # importing model
```
```py
kmeans = KMeans(n_clusters=5) # initializing the model
```
```py
kmeans.fit(X) # fitting(training) the model
```
```py
cluster_labels = kmeans.predict(X) # predict
```
```py
plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, edgecolors='k')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, marker='*', c='red')
```
```py
kmeans.cluster_centers_[:, 0] # accessing cluster centers
kmeans.cluster_centers_[:, 1]
```




## Confusion matrix 

A confusion matrix is a table used to describe the performance of a classification model. 
It shows how many predictions your model got right and wrong for each category, comparing the actual labels with the model’s predicted labels.


![Screenshot 2024-09-28 at 11 20 28](https://github.com/user-attachments/assets/fef18579-1cd9-4945-9d36-11afd8c54b7a)



![Screenshot 2024-09-28 at 11 35 10](https://github.com/user-attachments/assets/11e010e0-3545-4f25-9598-62348978b455)



![Screenshot 2024-09-28 at 11 38 56](https://github.com/user-attachments/assets/7c4f1b20-3689-4dcd-941a-30ceb5a0b57f)


### Data extraction

```py
import seaborn as sns
import pandas as pd 

titanic_data = sns.load_dataset('titanic')
```

### Data review

```py
print("Number of passangers:",len(titanic_data))
titanic_data.columns 
titanic_data.dtypes 

titanic_data['class'].unique()

titanic_data.isnull().sum()
```

### Data transformation

```py
titanic_data['deck'] = titanic_data['deck'].astype(str) 
titanic_data['class'] = titanic_data['class'].astype(str)
titanic_data.fillna(0, inplace=True)
titanic_data.drop(['alive', 'alone', 'adult_male', 'deck'], axis=1, inplace=True)

titanic_data.isnull().sum()

titanic_data['embark_town'].unique()
```

```py
# Mapping for embark_town
embark_town_mapping = {'Southampton': 1, 'Cherbourg': 2, 'Queenstown': 3, 0:0}
titanic_data['embark_town'] = titanic_data['embark_town'].map(embark_town_mapping)

# Mapping for sex
sex_mapping = {'male': 0, 'female': 1}
titanic_data['sex'] = titanic_data['sex'].map(sex_mapping)

# Mapping for embarked
embarked_mapping = {'S': 1, 'C': 2, 'Q': 3, 0 : 0}
titanic_data['embarked'] = titanic_data['embarked'].map(embarked_mapping)

# Mapping for class
class_mapping = {'Third': 1, 'First': 2, 'Second': 3}
titanic_data['class'] = titanic_data['class'].map(class_mapping)

# Mapping for who
who_mapping = {'man': 1, 'woman': 2, 'child': 3}
titanic_data['who'] = titanic_data['who'].map(who_mapping)
```

```py
titanic_data.isnull().sum() #NO MORE EMPTY VALUES
```

### Train and Test split

```py
from sklearn.model_selection import train_test_split
```
```py
X = titanic_data.drop('survived', axis=1) #Independent variables
y = titanic_data['survived'] #Dependent variable
```
```py
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 )
```

### Create data model 

```py
from sklearn.neighbors import KNeighborsClassifier # import the model
```
```py
model = KNeighborsClassifier() # initialize
```
```py
model.fit(X_train, y_train) # train(fit) the model
```
```py
prediction_knn = model.predict(X_test) # prediction on test dataset, because test dataset has not seen the model
```

### How good is the model?

```py
from sklearn.metrics import accuracy_score
```
```py
accuracy_score(prediction_knn, y_test)
```

### Confusion Matrix

```py
from sklearn.metrics import confusion_matrix
```
```py
cm = confusion_matrix(y_test, prediction_knn)
```
```py
print(cm)
```
```py
import seaborn as sns
import matplotlib.pyplot as plt
```
```py
# visualizing a heatmap from confusion matrix using seaborn

sns.heatmap(cm, annot = True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True Labels')
```

![Screenshot 2024-09-28 at 11 55 24](https://github.com/user-attachments/assets/826ec840-5ead-4722-bcbc-bd594247287d)



# TEAMWORK:

1. Look at what is Central Limit Theorem and try to prove/ visualize it using NumPy

   https://www.geeksforgeeks.org/python-central-limit-theorem/

2. Watch the movie The Great Hack (https://www.imdb.com/title/tt4736550/)  
or read about Cambridge Analytica (https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html)


Problem to solve : 
Conduct a simulation to show how the Central Limit Theorem can be used to approximate the probability of certain outcomes in a given dataset. For example, simulate rolling a die multiple times and calculate the distribution of the average outcome.

Our team tried two ways of solving this problem:

### 1

The code loops over different sample sizes and stores the sample means for each case.
The histograms visualize how the distribution of the sample mean becomes approximately normal as the number of rolls increases, which demonstrates the Central Limit Theorem.

```py
import numpy as np
import matplotlib.pyplot as plt

# number of samples (number of die rolls)
num_rolls = [1, 10, 50, 100]  
means = []  # list to store the sample means

# Simulate rolling a die multiple times
for rolls in num_rolls:
    # Generating the sample means for 1000 trials
    np.random.seed(42)
    x = [np.mean(np.random.randint(1, 7, rolls)) for _ in range(1000)]
    means.append(x)

# Plotting the histograms of sample means
fig, ax = plt.subplots(2, 2, figsize=(8, 8))
k = 0
for i in range(0, 2):
    for j in range(0, 2):
        ax[i, j].hist(means[k], bins=10, density=True, alpha=0.75, edgecolor = 'black')
        ax[i, j].set_title(f'Rolls = {num_rolls[k]}')
        k += 1

plt.tight_layout()
plt.show()
```

<img width="630" alt="Screenshot 2024-09-28 at 12 20 57" src="https://github.com/user-attachments/assets/f4617478-0a2c-4238-9d64-d956b2f620a2">


### 2

The first plot shows the original distribution of the die rolls, which should appear uniform across the 6 die faces.
The second plot shows the distribution of the sample means for the 1000 samples of 50 rolls each. According to the Central Limit Theorem, this distribution of sample means should approximate a normal distribution, centered around the expected mean of the die rolls (which is 3.5).
As you draw more samples and increase the sample size, the sample means will form a normal distribution, even though the underlying population is uniform.

```py
import numpy as np
import matplotlib.pyplot as plt

num_rolls = 10000  
population_dice = np.random.randint(1, 7, size=num_rolls) 

sample_size = 50  
num_samples = 1000  
sample_means_dice = []

for _ in range(num_samples):
    sample = np.random.choice(population_dice, size=sample_size)
    sample_means_dice.append(np.mean(sample))

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.hist(population_dice, bins=6, color='pink', edgecolor='black', rwidth=0.8)
plt.title("Original Die Roll Distribution (Uniform)")
plt.xlabel("Die Face Value")
plt.ylabel("Frequency")

plt.subplot(1, 2, 2)
plt.hist(sample_means_dice, bins=30, color='lightblue', edgecolor='black')
plt.title("Distribution of Sample Means (n=50, 1000 samples)")
plt.xlabel("Average Die Roll Value")
plt.ylabel("Frequency")

plt.tight_layout()
plt.show()
```

<img width="1259" alt="Screenshot 2024-09-28 at 12 20 46" src="https://github.com/user-attachments/assets/4f75a3e0-b27e-4e52-a562-6bf6e47c3497">
