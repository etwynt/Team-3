üî¢ Project 3: Titanic data
1. Using several classification models
2. Training classification models
3. Comparing classification models (accuracy score)
4. Confusion matrix

### DATA IMPORT

```py
import seaborn as sns
import pandas as pd

titanic_data = sns.load_dataset('titanic')

titanic_data.head()
```

### Data INVESTIGATION

```py
titanic_data.isnull().sum() #checking null values
titanic_data.dtypes
```

```py
titanic_data.drop('deck', axis=1, inplace=True) #dropping column with a lot of empty values
```

### Data Transformation

```py
titanic_data['class'] = titanic_data['class'].astype(str) # modified the object from category to string

titanic_data.fillna(0, inplace=True) # this command replaces empty with null values
```
```py
titanic_data.head()
```
```py
titanic_data['sex'].unique()
```
```py
titanic_data['who'].unique()
```
```py
# sex column

sex_mapping = {'male': 0, 'female': 1}
titanic_data['sex'] = titanic_data['sex'].map(sex_mapping)
```
```py

# Embark town

titanic_data['embark_town'].unique()
embark_town_mapping = {'Southampton': 1, 'Cherbourg': 2, 'Queenstown': 3, 0:0}
titanic_data['embark_town'] = titanic_data['embark_town'].map(embark_town_mapping)
```

```py
# Embarked
titanic_data['embarked'].unique()
embarked_mapping = {'S': 1, 'C': 2, 'Q': 3, 0:0}
titanic_data['embarked'] = titanic_data['embarked'].map(embarked_mapping)
```

```py
# Who
titanic_data['who'].unique()
who_mapping = {'man': 1, 'woman': 2, 'child': 3, 0:0}
titanic_data['who'] = titanic_data['who'].map(who_mapping)
```

```py
titanic_data.info()
```

### Data modeling ###

```py
from sklearn.model_selection import train_test_split

X = titanic_data.drop('survived', axis=1) # Features
y = titanic_data['survived'] # Dependent variable
```
```py
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)
```

### Logistic Regression ###

```py
from sklearn.linear_model import LogisticRegression #import
```
```py
model = LogisticRegression() #initialize
```
```py
model.fit(X_train, y_train) #train
```
```py
prediction_results = model.predict(X_test)
prediction_results
```

## TEAMWORK ##
EASY: add DecisionTreeClassifier to titanic data predictions. 

HARD: Investigate what is cross-validation and implement cross-validation on any classification model you prefer on Titanic data. Explain to each other, what do you see. 

Example: 
```py
k_fold = KFold(n_splits=10, shuffle=True, random_state=42)
accuracy_scores = cross_val_score(model_knn_cv, X, y, cv=k_fold, scoring='accuracy')
```
### DecisionTreeClassifier ###

```py
from sklearn.tree import DecisionTreeClassifier # import
model2 = DecisionTreeClassifier() # initialize
model2.fit(X_train, y_train) # train

prediction_results2 = model2.predict(X_test)
prediction_results2
```

### What is Cross-Validation? ###
**Cross-validation** is a statistical technique used to assess the performance and generalizability of a machine learning model. The basic idea is to divide the data into several subsets, train the model on some of these subsets, and test it on the remaining ones. This process is repeated multiple times to ensure that the model's performance is consistent across different splits of the data.

One of the most commonly used forms of cross-validation is **k-fold cross-validation**:

1. The dataset is split into ùëò equally sized folds (subsets).
2. The model is trained on ùëò ‚àí 1 folds and tested on the remaining one fold. This process is repeated ùëò times, with each fold being used exactly once as the test set.
3. The results from the ùëò iterations are averaged to produce a single estimation of the model‚Äôs performance.
   
This method provides a better estimation of model performance than a simple train-test split, as it reduces the variance associated with how the data is split.

```py
from sklearn.model_selection import KFold, cross_val_score

k_fold = KFold(n_splits=10, shuffle=True, random_state=42)
accuracy_scores = cross_val_score(model2, X, y, cv=k_fold, scoring='accuracy')

mean_accuracy = accuracy_scores.mean()
std_accuracy = accuracy_scores.std()

print(f"Mean Accuracy: {mean_accuracy}") # The mean accuracy across all folds gives an estimate of how well the model performs.
print(f"Standard Deviation of Accuracy: {std_accuracy}") # The standard deviation shows the variability in model performance across different folds.
print(f"All Accuracy Scores: {accuracy_scores}")
```

![image](https://github.com/user-attachments/assets/796fa4e1-ec62-46ab-b778-967924f09fa7)

**1. Mean Accuracy: 0.7845**
   
What it means:
The mean accuracy is the average accuracy of the model across all 10 folds in the cross-validation. An accuracy of 0.7845 means that, on average, the model correctly predicts whether a passenger survived 78.45% of the time.

Interpretation:
This suggests that the DecisionTreeClassifier model has a reasonably good performance on this dataset, as it correctly classifies the outcome for nearly 8 out of 10 passengers.

**2. Standard Deviation of Accuracy: 0.0322**
   
What it means:
The standard deviation measures how much the accuracy scores vary across the different folds. A standard deviation of 0.0322 indicates that the accuracies in the 10 folds differ by about 3.22% from the mean accuracy on average.

Interpretation:
The relatively low standard deviation suggests that the model's performance is consistent across different splits of the data. This is a good sign as it implies that the model generalizes well to different subsets of the data and isn't overly sensitive to the specific training data used in each fold.

**3. All Accuracy Scores:**
[0.8, 0.80898876, 0.79775281, 0.71910112, 0.82022472, 0.7752809, 0.74157303, 0.79775281, 0.76404494, 0.82022472]

What it means:
These are the individual accuracy scores from each of the 10 folds.

Interpretation:
The scores vary from a low of 0.7191 (71.91%) to a high of 0.8202 (82.02%).
While there is some variation between the folds, most accuracy scores are clustered around the 78-82% range, indicating consistent performance across different portions of the dataset.
The lower score (71.91%) might indicate that in some folds, the data split resulted in a slightly more challenging subset, perhaps with less representative data for training or more complex patterns to learn.

**Overall Interpretation:**
The results suggest that the DecisionTreeClassifier performs well on the Titanic dataset, with consistent accuracy across different data splits. The mean accuracy of about 78.45% is solid, indicating that the model is reasonably effective at predicting survival. The low standard deviation shows that this performance is stable and not heavily dependent on the specific training data used, which is a positive indication of the model's generalizability.
